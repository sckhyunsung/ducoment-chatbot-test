import os
import time
import tempfile
import email
from email import policy
from typing import List, Tuple, Optional

import pandas as pd
import streamlit as st
from bs4 import BeautifulSoup

# =============================================================
# Environment tweaks
# =============================================================
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# =============================================================
# LangChain-compatible imports with robust fallbacks
# =============================================================
# Text Splitter (new package -> newer LC namespace -> old LC namespace)
try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter  # newest
    _SPLITTER_SRC = "langchain_text_splitters"
except Exception:
    try:
        from langchain.text_splitters import RecursiveCharacterTextSplitter  # newer LC
        _SPLITTER_SRC = "langchain.text_splitters"
    except Exception:
        from langchain.text_splitter import RecursiveCharacterTextSplitter  # old LC
        _SPLITTER_SRC = "langchain.text_splitter"

# VectorStore FAISS (community -> old)
try:
    from langchain_community.vectorstores import FAISS
except Exception:
    from langchain.vectorstores import FAISS  # very old fallback

# Embeddings & Chat LLM (langchain_openai)
try:
    from langchain_openai import OpenAIEmbeddings, ChatOpenAI
except Exception as e:
    st.error("âŒ 'langchain_openai'ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. requirements.txtì— 'langchain-openai>=0.2.0'ì„ ì¶”ê°€í•˜ì„¸ìš”.")
    raise

# Document loaders (prefer community; fallback to old; allow None)
try:
    from langchain_community.document_loaders import (
        PyPDFLoader,
        TextLoader,
        UnstructuredWordDocumentLoader,
        UnstructuredPowerPointLoader,
    )
except Exception:
    try:
        from langchain.document_loaders import (
            PyPDFLoader,
            TextLoader,
            UnstructuredWordDocumentLoader,
            UnstructuredPowerPointLoader,
        )
    except Exception:
        PyPDFLoader = None
        TextLoader = None
        UnstructuredWordDocumentLoader = None
        UnstructuredPowerPointLoader = None

# Prompts
from langchain.prompts import ChatPromptTemplate

# Tool & Agent (newer paths first)
try:
    from langchain.tools import Tool
except Exception:
    try:
        from langchain.agents import Tool  # very old
    except Exception:
        Tool = None

try:
    from langchain.agents import create_tool_calling_agent, AgentExecutor
    _AGENTS_AVAILABLE = True
except Exception:
    # Fallback: we'll run a manual RAG pipeline (no Agent)
    create_tool_calling_agent = None
    AgentExecutor = None
    _AGENTS_AVAILABLE = False

# =============================================================
# Utility: simple banner for current import modes (helps debugging)
# =============================================================
def _debug_imports_banner():
    st.caption(
        f"ğŸ”§ Imports â€” Splitter: `{_SPLITTER_SRC}`, Agent: "
        + ("ON (create_tool_calling_agent)" if _AGENTS_AVAILABLE else "OFF (manual RAG)")
    )

# =============================================================
# Document loading helpers
# =============================================================
class SimpleDocument:
    """Lightweight fallback if LC Document import path ever changes."""
    def __init__(self, page_content: str, metadata: Optional[dict] = None):
        self.page_content = page_content
        self.metadata = metadata or {}

# Prefer official LC Document class, but fall back if needed
try:
    from langchain_core.documents import Document as LCDocument
except Exception:
    try:
        from langchain.schema import Document as LCDocument
    except Exception:
        LCDocument = None

DocumentCls = LCDocument if LCDocument is not None else SimpleDocument


def _read_txt_fallback(path: str) -> str:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    except Exception:
        with open(path, "r", encoding="cp949", errors="ignore") as f:
            return f.read()


# -------------------------------------------------------------
# Load various file types into a list[Document]
# -------------------------------------------------------------
def load_documents(uploaded_files) -> List[DocumentCls]:
    all_documents: List[DocumentCls] = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    total_files = len(uploaded_files)

    for idx, uploaded_file in enumerate(uploaded_files):
        status_text.text(f"ğŸ“‚ íŒŒì¼ ì²˜ë¦¬ ì¤‘: {uploaded_file.name} ({idx+1}/{total_files})")
        progress_bar.progress((idx + 1) / total_files)

        file_extension = uploaded_file.name.split(".")[-1].lower()

        # Make a temp file
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_extension}") as tmp_file:
            tmp_file.write(uploaded_file.read())
            tmp_file_path = tmp_file.name

        try:
            if file_extension == "pdf":
                if PyPDFLoader is None:
                    st.warning("âš ï¸ PyPDFLoaderë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'langchain_community' ë˜ëŠ” í˜¸í™˜ ë²„ì „ì„ ì„¤ì¹˜í•˜ì„¸ìš”.")
                    continue
                loader = PyPDFLoader(tmp_file_path)
                documents = loader.load()

            elif file_extension == "txt":
                # Use LC TextLoader if available, otherwise simple read
                if TextLoader is not None:
                    loader = TextLoader(tmp_file_path, encoding="utf-8")
                    documents = loader.load()
                    for d in documents:
                        d.metadata["source"] = uploaded_file.name
                else:
                    text = _read_txt_fallback(tmp_file_path)
                    documents = [DocumentCls(page_content=text, metadata={"source": uploaded_file.name, "type": "Text"})]

            elif file_extension == "csv":
                df = pd.read_csv(tmp_file_path)
                text = df.to_string()
                documents = [DocumentCls(page_content=text, metadata={"source": uploaded_file.name, "type": "CSV"})]

            elif file_extension in ["xlsx", "xls"]:
                excel_file = pd.ExcelFile(tmp_file_path)
                documents = []
                for sheet_name in excel_file.sheet_names:
                    df = pd.read_excel(tmp_file_path, sheet_name=sheet_name)
                    text = f"Sheet: {sheet_name}\n\n{df.to_string()}"
                    documents.append(
                        DocumentCls(
                            page_content=text,
                            metadata={"source": uploaded_file.name, "sheet": sheet_name, "type": "Excel"},
                        )
                    )

            elif file_extension in ["docx", "doc"]:
                if UnstructuredWordDocumentLoader is None:
                    st.warning("âš ï¸ Word ë¡œë”ê°€ ì—†ìŒ: 'unstructured' ê´€ë ¨ ì˜ì¡´ì„±ì„ ì„¤ì¹˜í•˜ì„¸ìš” (ë˜ëŠ” Wordë¥¼ ë‹¤ë¥¸ í¬ë§·ìœ¼ë¡œ ë³€í™˜).")
                    continue
                loader = UnstructuredWordDocumentLoader(tmp_file_path)
                documents = loader.load()
                for d in documents:
                    d.metadata["source"] = uploaded_file.name
                    d.metadata["type"] = "Word"

            elif file_extension in ["pptx", "ppt"]:
                if UnstructuredPowerPointLoader is None:
                    st.warning("âš ï¸ PowerPoint ë¡œë”ê°€ ì—†ìŒ: 'unstructured' ê´€ë ¨ ì˜ì¡´ì„±ì„ ì„¤ì¹˜í•˜ì„¸ìš” (ë˜ëŠ” PPTë¥¼ PDFë¡œ ì €ì¥ í›„ ì—…ë¡œë“œ).")
                    continue
                loader = UnstructuredPowerPointLoader(tmp_file_path)
                documents = loader.load()
                for d in documents:
                    d.metadata["source"] = uploaded_file.name
                    d.metadata["type"] = "PowerPoint"

            elif file_extension in ["mhtml", "mht"]:
                try:
                    with open(tmp_file_path, "rb") as f:
                        msg = email.message_from_binary_file(f, policy=policy.default)

                    text_parts = []
                    for part in msg.walk():
                        content_type = part.get_content_type()
                        if content_type in ["text/html", "text/plain"]:
                            payload = part.get_payload(decode=True)
                            if not payload:
                                continue
                            charset = part.get_content_charset() or "utf-8"
                            try:
                                text = payload.decode(charset, errors="ignore")
                            except Exception:
                                text = payload.decode("utf-8", errors="ignore")
                            if content_type == "text/html":
                                soup = BeautifulSoup(text, "html.parser")
                                text = soup.get_text(separator="\n", strip=True)
                            text_parts.append(text)

                    if text_parts:
                        combined_text = "\n\n".join(text_parts)
                        documents = [DocumentCls(page_content=combined_text, metadata={"source": uploaded_file.name, "type": "MHTML"})]
                    else:
                        st.warning(f"âš ï¸ MHTML í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {uploaded_file.name}")
                        continue
                except Exception as e:
                    st.error(f"âŒ MHTML ì²˜ë¦¬ ì˜¤ë¥˜: {uploaded_file.name} - {str(e)}")
                    continue

            else:
                st.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {uploaded_file.name}")
                continue

            # Normalize metadata for LC vs SimpleDocument
            norm_docs = []
            for d in documents:
                # Some loaders may yield dict-like objects in very old versions
                if hasattr(d, "page_content"):
                    content = d.page_content
                    meta = getattr(d, "metadata", {}) or {}
                else:
                    try:
                        content = d["page_content"]
                        meta = d.get("metadata", {}) or {}
                    except Exception:
                        continue
                norm_docs.append(DocumentCls(page_content=content, metadata=meta))

            all_documents.extend(norm_docs)

        except Exception as e:
            st.error(f"âŒ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({uploaded_file.name}): {str(e)}")
        finally:
            if os.path.exists(tmp_file_path):
                os.unlink(tmp_file_path)

    progress_bar.empty()
    status_text.empty()
    return all_documents


# -------------------------------------------------------------
# Vector store creation and retrieval tool
# -------------------------------------------------------------
def create_vectorstore(uploaded_files):
    status_container = st.empty()

    try:
        # Step 1: Load
        status_container.info("ğŸ”„ **1ë‹¨ê³„**: ë¬¸ì„œ íŒŒì¼ì„ ì½ê³  ìˆìŠµë‹ˆë‹¤...")
        all_documents = load_documents(uploaded_files)
        if not all_documents:
            status_container.error("âŒ ë¬¸ì„œë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # Step 2: Split
        status_container.info(f"ğŸ”„ **2ë‹¨ê³„**: ë¬¸ì„œë¥¼ {len(all_documents)}ê°œ í•­ëª©ìœ¼ë¡œ ë¶„í•  ì¤‘...")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        split_docs = text_splitter.split_documents(all_documents)

        # Step 3: Embed
        status_container.info(
            f"ğŸ”„ **3ë‹¨ê³„**: {len(split_docs)}ê°œ ì²­í¬ ì„ë² ë”© ì¤‘... (OpenAI API í˜¸ì¶œ)"
        )
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        vector = FAISS.from_documents(split_docs, embeddings)
        retriever = vector.as_retriever(search_kwargs={"k": 5})

        status_container.success(
            f"âœ… **ì™„ë£Œ!** {len(uploaded_files)}ê°œ íŒŒì¼, {len(split_docs)}ê°œ ì²­í¬ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!"
        )
        time.sleep(1)
        status_container.empty()

        def _format_location(md: dict) -> str:
            page = md.get("page", "")
            sheet = md.get("sheet", "")
            doc_type = md.get("type", "")
            if page != "":
                return f"{page}í˜ì´ì§€"
            if sheet:
                return f"ì‹œíŠ¸: {sheet}"
            if doc_type:
                return f"{doc_type} ë¬¸ì„œ"
            return ""

        def _retrieve(query: str):
            try:
                return retriever.get_relevant_documents(query)
            except Exception:
                return []

        def _format_docs_md(docs) -> str:
            if not docs:
                return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            result_text = "### ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©:\n\n"
            for idx, doc in enumerate(docs, 1):
                meta = getattr(doc, "metadata", {}) or {}
                source = meta.get("source", "ì•Œ ìˆ˜ ì—†ìŒ")
                location = _format_location(meta)
                snippet = (doc.page_content or "").strip().replace("\n", " ")[:500]
                result_text += f"**[ë¬¸ì„œ {idx}]**\n"
                result_text += f"- ğŸ“„ íŒŒì¼ëª…: {source}\n"
                if location:
                    result_text += f"- ğŸ“ ìœ„ì¹˜: {location}\n"
                result_text += f"- ğŸ“ ë‚´ìš©: {snippet}...\n\n"
                result_text += "---\n\n"
            return result_text

        # Tool callable (for Agent mode)
        def search_documents(query: str) -> str:
            docs = _retrieve(query)
            return _format_docs_md(docs)

        if Tool is None:
            retriever_tool = None
        else:
            retriever_tool = Tool(
                name="document_search",
                func=search_documents,
                description=(
                    "ì—…ë¡œë“œëœ ë¬¸ì„œ(PDF, Excel, Word, PowerPoint, Text, CSV)ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. "
                    "ê²€ìƒ‰ ê²°ê³¼ëŠ” íŒŒì¼ëª…, í˜ì´ì§€/ì‹œíŠ¸ ì •ë³´ì™€ í•¨ê»˜ ë°˜í™˜ë©ë‹ˆë‹¤. "
                    "ë°˜ë“œì‹œ ì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì„¸ìš”."
                ),
            )

        # Provide everything we might need later
        return {
            "tool": retriever_tool,
            "retriever": retriever,
            "format_docs_md": _format_docs_md,
        }

    except Exception as e:
        status_container.error(f"âŒ ë²¡í„° DB ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None


# =============================================================
# UI helpers
# =============================================================

def print_messages():
    for msg in st.session_state["messages"]:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"]) 


# =============================================================
# Main app
# =============================================================

def main():
    st.set_page_config(
        page_title="SCK ì±—ë´‡ Test ë„ìš°ë¯¸",
        layout="wide",
        page_icon="ğŸ¤–",
        initial_sidebar_state="expanded",
    )

    # Header
    with st.container():
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            if os.path.exists("./chatbot_logo.jpg"):
                st.image("./chatbot_logo.jpg", width=600)
            elif os.path.exists("chatbot_logo.jpg"):
                st.image("chatbot_logo.jpg", width=600)
        st.markdown("---")
        st.markdown("# SCK ì±—ë´‡ ê°œë°œ ë„ìš°ë¯¸")
        st.markdown("### Documentë¥¼ Uploadí•˜ì—¬ Testí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        st.markdown("**ë¬¸ì˜ ì‚¬í•­ì€ ê¸°ìˆ íŒ€ ì´í˜„ì„± ì„ ì„ì—ê²Œ ì§„í–‰í•´ ì£¼ì„¸ìš”.**")
        st.markdown("**Contact:** íŒ€ì¦ˆ or Mail ([hyunsung.lee@statschippac.com](mailto:hyunsung.lee@statschippac.com))")
        st.markdown("---")

    # Session state init
    if "messages" not in st.session_state:
        st.session_state["messages"] = []
    if "vectorstore_ready" not in st.session_state:
        st.session_state["vectorstore_ready"] = False
    if "agent_executor" not in st.session_state:
        st.session_state["agent_executor"] = None
    if "is_processing" not in st.session_state:
        st.session_state["is_processing"] = False
    if "pending_question" not in st.session_state:
        st.session_state["pending_question"] = None

    # Sidebar
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        st.session_state["OPENAI_API"] = st.text_input(
            "OPENAI API í‚¤",
            placeholder="sk-...",
            type="password",
            help="OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
        )

        st.markdown("---")
        st.header("ğŸ“ ë¬¸ì„œ ì—…ë¡œë“œ")
        with st.expander("ğŸ“‹ ì§€ì› íŒŒì¼ í˜•ì‹", expanded=False):
            st.markdown(
                """
                - ğŸ“• PDF (.pdf)
                - ğŸ“Š Excel (.xlsx, .xls)
                - ğŸ“˜ Word (.docx, .doc)
                - ğŸ¨ PowerPoint (.pptx, .ppt)
                - ğŸŒ MHTML (.mhtml, .mht)
                - ğŸ“„ Text (.txt)
                - ğŸ“‘ CSV (.csv)
                """
            )

        uploaded_files = st.file_uploader(
            "íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
            accept_multiple_files=True,
            type=[
                "pdf",
                "xlsx",
                "xls",
                "docx",
                "doc",
                "pptx",
                "ppt",
                "mhtml",
                "mht",
                "txt",
                "csv",
            ],
            key="file_uploader",
            help="ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
        )

        if uploaded_files:
            st.success(f"âœ… {len(uploaded_files)}ê°œì˜ íŒŒì¼ ì„ íƒë¨")
            with st.expander("ğŸ“‚ ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡", expanded=True):
                total_size = 0
                for file in uploaded_files:
                    file_size = len(file.getvalue()) / 1024  # KB
                    total_size += file_size
                    file_icon = {
                        "pdf": "ğŸ“•",
                        "xlsx": "ğŸ“Š",
                        "xls": "ğŸ“Š",
                        "docx": "ğŸ“˜",
                        "doc": "ğŸ“˜",
                        "pptx": "ğŸ¨",
                        "ppt": "ğŸ¨",
                        "mhtml": "ğŸŒ",
                        "mht": "ğŸŒ",
                        "txt": "ğŸ“„",
                        "csv": "ğŸ“‘",
                    }.get(file.name.split(".")[-1].lower(), "ğŸ“„")
                    st.markdown(f"{file_icon} **{file.name}**")
                    st.caption(f"í¬ê¸°: {file_size:.1f} KB")
                st.markdown(f"**ì´ ìš©ëŸ‰**: {total_size:.1f} KB")

        if st.session_state["vectorstore_ready"]:
            st.markdown("---")
            st.header("ğŸ“Š ìƒíƒœ")
            st.metric("ëŒ€í™” ìˆ˜", len(st.session_state["messages"]) // 2)
            if st.button("ğŸ—‘ï¸ ëŒ€í™” ë‚´ì—­ ì´ˆê¸°í™”"):
                st.session_state["messages"] = []
                st.rerun()

    if not st.session_state["OPENAI_API"]:
        st.warning("âš ï¸ ì‚¬ì´ë“œë°”ì—ì„œ OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.info("ğŸ’¡ API í‚¤ëŠ” ê¸°ìˆ íŒ€ ì´í˜„ì„± ì„ ì„ì—ê²Œ ë¬¸ì˜ ë¶€íƒë“œë¦½ë‹ˆë‹¤.")
        return

    # Ensure OpenAI key in env before any embedding/LLM call
    os.environ["OPENAI_API_KEY"] = st.session_state["OPENAI_API"]

    if not uploaded_files:
        st.info("ğŸ“„ ì‚¬ì´ë“œë°”ì—ì„œ ë¬¸ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        st.markdown(
            """
            ### ğŸ’¡ ì‚¬ìš© ë°©ë²•
            1. ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ OpenAI API í‚¤ ì…ë ¥
            2. ë¶„ì„í•˜ê³  ì‹¶ì€ ë¬¸ì„œ íŒŒì¼ ì—…ë¡œë“œ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)
            3. ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ í›„ ì§ˆë¬¸ ì…ë ¥
            4. AIê°€ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ì œê³µ

            ### âš ï¸ ì£¼ì˜ì‚¬í•­
            - ì—…ë¡œë“œëœ ë¬¸ì„œëŠ” OpenAI ì„œë²„ë¡œ ì „ì†¡ë©ë‹ˆë‹¤
            - ê¸°ë°€ ë¬¸ì„œëŠ” ì—…ë¡œë“œí•˜ì§€ ë§ˆì„¸ìš”
            - ì„¸ì…˜ ì¢…ë£Œ ì‹œ ëª¨ë“  ë°ì´í„°ê°€ ì‚­ì œë©ë‹ˆë‹¤
            """
        )
        return

    # Detect file changes to rebuild vectorstore
    current_file_names = [f.name for f in uploaded_files]
    if "previous_files" not in st.session_state:
        st.session_state["previous_files"] = []

    if current_file_names != st.session_state["previous_files"]:
        st.session_state["vectorstore_ready"] = False
        st.session_state["previous_files"] = current_file_names
        st.session_state["messages"] = []

    # Build vectorstore if needed
    if not st.session_state["vectorstore_ready"]:
        st.info("ğŸ”„ ë¬¸ì„œ ì—…ë¡œë“œë¥¼ ê°ì§€í–ˆìŠµë‹ˆë‹¤. ë²¡í„° ì„ë² ë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        vs = create_vectorstore(uploaded_files)
        if vs is None:
            st.error("âŒ ë¬¸ì„œ ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return

        # LLM init (support both model and model_name kwargs across versions)
        try:
            llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        except TypeError:
            llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

        st.session_state["llm"] = llm
        st.session_state["retriever"] = vs["retriever"]
        st.session_state["format_docs_md"] = vs["format_docs_md"]

        if _AGENTS_AVAILABLE and vs["tool"] is not None:
            prompt = ChatPromptTemplate.from_messages(
                [
                    (
                        "system",
                        "ë‹¹ì‹ ì€ 'SCK ì±—ë´‡ Test ë„ìš°ë¯¸'ì…ë‹ˆë‹¤.\n\n"
                        "# ì¤‘ìš”: ë„êµ¬ ì‚¬ìš© í•„ìˆ˜\n"
                        "ì§ˆë¬¸ì„ ë°›ìœ¼ë©´ ë°˜ë“œì‹œ ë¨¼ì € document_search ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì—¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•´ì•¼ í•©ë‹ˆë‹¤.\n"
                        "ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë‹µë³€í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.\n\n"
                        "# ë‹µë³€ í˜•ì‹\n"
                        "## ìš”ì•½\ní•µì‹¬ ë‚´ìš© ê°„ë‹¨íˆ ì„¤ëª…\n\n"
                        "## ìƒì„¸ ë‚´ìš©\n- ë°ì´í„°ëŠ” í‘œë¡œ ì •ë¦¬\n- ìˆ˜ì¹˜ëŠ” ì •í™•í•˜ê²Œ (ë‹¨ìœ„ í¬í•¨)\n- ë‚ ì§œ/ì‹œê°„ì€ ì›ë³¸ í˜•ì‹ ìœ ì§€\n\n"
                        "## ì°¸ê³  ë¬¸ì„œ\níŒŒì¼ëª…ê³¼ ìœ„ì¹˜ ì •ë³´\n\n"
                        "# ê·œì¹™\n- ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡ ê¸ˆì§€\n- í•­ìƒ í•œêµ­ì–´ë¡œ ë‹µë³€",
                    ),
                    ("human", "{input}"),
                    ("placeholder", "{agent_scratchpad}"),
                ]
            )
            agent = create_tool_calling_agent(st.session_state["llm"], [vs["tool"]], prompt)
            agent_executor = AgentExecutor(
                agent=agent,
                tools=[vs["tool"]],
                verbose=True,
                handle_parsing_errors=True,
                max_iterations=15,
                return_intermediate_steps=True,
            )
            st.session_state["agent_executor"] = agent_executor
        else:
            # Manual RAG fallback (no Agent): we'll compose our own prompt using retrieved context
            st.session_state["agent_executor"] = None

        # Celebration
        st.balloons()
        time.sleep(0.3)
        st.snow()
        st.success("ğŸ‰ ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        time.sleep(0.6)
        _debug_imports_banner()
        st.session_state["vectorstore_ready"] = True
        st.rerun()

    # Chat UI
    if st.session_state["vectorstore_ready"]:
        user_input = st.chat_input("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
        if user_input and not st.session_state["is_processing"]:
            st.session_state["is_processing"] = True
            st.session_state["pending_question"] = user_input
            st.rerun()

        if st.session_state["is_processing"]:
            st.warning("â³ ë‹µë³€ ìƒì„± ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")

        print_messages()

        if st.session_state["pending_question"]:
            question = st.session_state["pending_question"]
            st.session_state["pending_question"] = None

            st.session_state["messages"].append({"role": "user", "content": question})
            with st.chat_message("user"):
                st.markdown(question)

            with st.chat_message("assistant"):
                status_placeholder = st.empty()
                status_placeholder.info("ğŸ” ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")

                try:
                    if st.session_state["agent_executor"] is not None:
                        # Agent path (will call the tool itself)
                        status_placeholder.info("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘ (ì—ì´ì „íŠ¸)...")
                        result = st.session_state["agent_executor"].invoke({"input": question})
                        response = result["output"]
                        status_placeholder.empty()
                        st.markdown(response)
                        st.session_state["messages"].append({"role": "assistant", "content": response})
                    else:
                        # Manual RAG path
                        status_placeholder.info("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘ (ìˆ˜ë™ RAG)...")
                        retriever = st.session_state["retriever"]
                        docs = retriever.get_relevant_documents(question)
                        # Compose context text (compact)
                        context_snippets = []
                        for d in docs:
                            src = (d.metadata or {}).get("source", "")
                            snippet = (d.page_content or "").strip().replace("\n", " ")[:700]
                            context_snippets.append(f"[source: {src}] {snippet}")
                        context_text = "\n\n".join(context_snippets) if context_snippets else "(ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)"

                        manual_prompt = ChatPromptTemplate.from_messages(
                            [
                                (
                                    "system",
                                    "ë‹¹ì‹ ì€ ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œë§Œ ê·¼ê±°ë¥¼ ì°¾ëŠ” í•œêµ­ì–´ ì „ë¬¸ê°€ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
                                    "ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  'ë¬¸ì„œì— ê·¼ê±°ê°€ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ì„¸ìš”.\n"
                                    "ìš”ì²­ëœ í¬ë§·(ìš”ì•½/ìƒì„¸/ì°¸ê³  ë¬¸ì„œ)ì„ ë°˜ë“œì‹œ ì§€í‚¤ì„¸ìš”.",
                                ),
                                (
                                    "human",
                                    "ì§ˆë¬¸: {question}\n\n"
                                    "ì»¨í…ìŠ¤íŠ¸(ê²€ìƒ‰ ê²°ê³¼ ë°œì·Œ):\n{context}\n\n"
                                    "ìœ„ ì»¨í…ìŠ¤íŠ¸ë§Œ ê·¼ê±°ë¡œ ë‹µí•˜ì„¸ìš”.",
                                ),
                            ]
                        )
                        try:
                            llm = st.session_state["llm"]
                        except KeyError:
                            try:
                                llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
                            except TypeError:
                                llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

                        messages = manual_prompt.format_messages(question=question, context=context_text)
                        ai_msg = llm.invoke(messages)
                        response = ai_msg.content if hasattr(ai_msg, "content") else str(ai_msg)

                        # Add reference block (file and location list)
                        def _format_location(md: dict) -> str:
                            page = md.get("page", "")
                            sheet = md.get("sheet", "")
                            doc_type = md.get("type", "")
                            if page != "":
                                return f"{page}í˜ì´ì§€"
                            if sheet:
                                return f"ì‹œíŠ¸: {sheet}"
                            if doc_type:
                                return f"{doc_type} ë¬¸ì„œ"
                            return ""

                        refs_md_lines = ["\n\n## ì°¸ê³  ë¬¸ì„œ"]
                        if docs:
                            for i, d in enumerate(docs, 1):
                                md = d.metadata or {}
                                src = md.get("source", "ì•Œ ìˆ˜ ì—†ìŒ")
                                loc = _format_location(md)
                                refs_md_lines.append(f"- [{i}] {src} " + (f"- {loc}" if loc else ""))
                        else:
                            refs_md_lines.append("- (ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)")

                        full_response = response + "\n" + "\n".join(refs_md_lines)

                        status_placeholder.empty()
                        st.markdown(full_response)
                        st.session_state["messages"].append({"role": "assistant", "content": full_response})

                except Exception as e:
                    status_placeholder.empty()
                    error_msg = f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n\n```{str(e)}```\n\në‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                    st.error(error_msg)
                    st.session_state["messages"].append({"role": "assistant", "content": error_msg})

                finally:
                    st.session_state["is_processing"] = False
                    time.sleep(0.2)
                    st.rerun()


if __name__ == "__main__":
    main()
